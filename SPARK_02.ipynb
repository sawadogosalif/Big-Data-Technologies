{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "+ Hadoop est un framework pour le stockage distribué ( HDFS ) et le traitement distribué ( YARN ).\n",
    "\n",
    "+ **Apache Hive** is un data warehouse et outil  ETL  au dessus de la Apache Hadoop,  nous y avons crée nos bases de dedons. Donc nous avons besoin d'un context hive\n",
    "\n",
    "+ Spark est un moteur de calcul distribué en mémoire.\n",
    "\n",
    "+ Spark peut fonctionner avec ou sans composants Hadoop (HDFS/YARN)\n",
    "\n",
    "+ Spark n'est pas lié au paradigme MapReduce en deux étapes et promet des performances jusqu'à 100 fois plus rapides que Hadoop MapReduce pour certaines applications\n",
    "\n",
    "+ Le SparkContext peut se connecter à plusieurs types de cluster manager (local, Mesos ou YARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, HiveContext, SparkConf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "Ceci est  pour exécuter le travail en mode local.\n",
    "\n",
    "Il est utilisé pour tester le code dans une petite quantité de données dans un environnement local\n",
    "\n",
    "Il ne fournit pas assez d' avantages de l'environnement distribué surtout dans la gestions des ressources\n",
    "\n",
    "*  \"*\" est le nombre de cores à disponible à alloueer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster('local[*]')                # setMaster('yarn-client')\n",
    "conf = conf.setAppName('my_second_app')\n",
    "conf =  conf.set('spark.ui.port', '5050')               # definition du uri spark\n",
    "conf= conf.set('spark.sql.shuffle.partitions','4')\n",
    "sc = SparkContext(conf=conf)                            # Instanciation du context spark\n",
    "hctx = HiveContext(sc)                                  # Instanciation du context hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()                                             # arret du context spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture de la table de donnée depuis Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = hctx.sql(\"SELECT  idnt_comp_serv, flg_rsil, anc_actv, \\\n",
    "                      val_debit_down, libl_res, libl_tech, nb_iad, iad_anc_info, iad_userkey2, fdate_iad_anc, \\\n",
    "                      iad_model, iad_vendor,     iad_softwarever, rk_asc_iad_conf, nb_stv, stb_anc_info, \\\n",
    "                      fdate_stb_anc, stb_model, stb_vendor, stb_softwarever, rk_asc_stb_conf, noinadinfo, nostbinfo \\\n",
    "              FROM upec_2022.iad_stb_model A  WHERE rdn_samp=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+--------------+--------+---------+------+------------+------------+-------------+---------+----------+--------------------+---------------+------+------------+-------------+--------------+----------+---------------+---------------+----------+---------+\n",
      "|idnt_comp_serv|flg_rsil|anc_actv|val_debit_down|libl_res|libl_tech|nb_iad|iad_anc_info|iad_userkey2|fdate_iad_anc|iad_model|iad_vendor|     iad_softwarever|rk_asc_iad_conf|nb_stv|stb_anc_info|fdate_stb_anc|     stb_model|stb_vendor|stb_softwarever|rk_asc_stb_conf|noinadinfo|nostbinfo|\n",
      "+--------------+--------+--------+--------------+--------+---------+------+------------+------------+-------------+---------+----------+--------------------+---------------+------+------------+-------------+--------------+----------+---------------+---------------+----------+---------+\n",
      "|  610004644264|       0|       1|       20000.0|      BT|     xDSL|     1|           0|          3P|            0| TVW620.I|      UBEE|B400FB_RECO_9.1.9...|              2|     1|           1|            1|RTI422-320_BYT|     Sagem|      G07.67.06|              2|         0|        0|\n",
      "|  610001492507|       0|      40|        2432.0|      FT|     xDSL|     1|           3|          2P|            6|    TG787|   Thomson|  B11001_MAIN_8.6.62|              5|  null|        null|         null|             ?|         ?|              ?|           null|         0|        0|\n",
      "+--------------+--------+--------+--------------+--------+---------+------+------------+------------+-------------+---------+----------+--------------------+---------------+------+------------+-------------+--------------+----------+---------------+---------------+----------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les classses sont très déséquilibres, le taux d'erreur n'est plus pertinent pour mésurer la qualité du modèle.\n",
    "\n",
    "En effet il y a 98% et 2%. Une des solutions serait de rééchantillonner les données. \n",
    "\n",
    "C'est pourquoi df revient à selectionner les rdn_samp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|FLG_RSIL|count(1)|\n",
      "+--------+--------+\n",
      "|       0|  601449|\n",
      "|       1|    8495|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hctx.sql(\"SELECT FLG_RSIL, COUNT(*) FROM upec_2022.iad_stb_model GROUP BY FLG_RSIL\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La table qui nous interesse est alors un tirage aléatoire de 100 % parmi les postitfs et 10 % parmi les negatifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|FLG_RSIL|count(1)|\n",
      "+--------+--------+\n",
      "|       0|   60230|\n",
      "|       1|    8495|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hctx.sql(\"SELECT FLG_RSIL, COUNT(*)    \\\n",
    "          FROM upec_2022.iad_stb_model \\\n",
    "          WHERE rdn_samp =1            \\\n",
    "         GROUP BY FLG_RSIL\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show() est une méthode qui s'applique au df, le hive context converti toute les tables en dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(hctx.sql(\"SELECT FLG_RSIL, COUNT(*)    \\\n",
    "          FROM upec_2022.iad_stb_model \\\n",
    "          WHERE rdn_samp =1            \\\n",
    "         GROUP BY FLG_RSIL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controle du nombre de partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD (Resilient Distributed Dataset) est un objet natif de spark.\n",
    "\n",
    "C' est une structure de données en mémoire utilisée par Spark. très rapide car plus besoin d'aller lire les données sur hdfs \n",
    "\n",
    "Une fois que le context spark  s'arrette ', il n'y a plus de RDD\n",
    "\n",
    " On peut changer le partitonning du dataset pour s'adapter à la puissance qu'on a, je n'ai que quatre cpus logiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = df.rdd.repartition(4)\n",
    "type(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les méthodes take() , collect() , first renvoient des objets python (list, row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd.take(2)), type(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le **lazy execution** dans Spark signifie que l'exécution ne démarrera pas tant qu'une action ne sera pas déclenchée.  Par exemple, df.rdd va d'abord revenir en arrière sur le code qui permet d'avoir df, une fois exécutée, alors on le transform en rdd. Donc cela est très couteux en temps de calcul.\n",
    "\n",
    "**Cache()**  et **persist()** sont des techniques d'optimisation pour les applications Spark  afin d'améliorer les performances des jos  des applications.Ils sont sont utilisés pour enregistrer les Spark RDD, Dataframe et Dataset.\n",
    "\n",
    "Mais, la différence est que la méthode RDD cache() l'enregistre par défaut dans la mémoire (MEMORY_ONLY) tandis que la méthode persist() est utilisée pour le stocker au niveau de stockage défini par l'utilisateur.\n",
    "\n",
    "Lorsque l'objet à persister est un rdd alors cache () et persist() deviennent pareils. Naturellement rdd.unpersist() existe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(idnt_comp_serv=610001492507, flg_rsil=0, anc_actv=40, val_debit_down=2432.0, libl_res=u'FT', libl_tech=u'xDSL', nb_iad=1, iad_anc_info=3, iad_userkey2=u'2P', fdate_iad_anc=6, iad_model=u'TG787', iad_vendor=u'Thomson', iad_softwarever=u'B11001_MAIN_8.6.62', rk_asc_iad_conf=5, nb_stv=None, stb_anc_info=None, fdate_stb_anc=None, stb_model=u'?', stb_vendor=u'?', stb_softwarever=u'?', rk_asc_stb_conf=None, noinadinfo=0, nostbinfo=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gestion de doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONTROLE ET GESTION DES DOUBLONS\n",
    "NonUnique = rdd.map(lambda x:  (int(x.idnt_comp_serv),1))\\\n",
    "                .reduceByKey(lambda x,y : x+y)\\\n",
    "                .filter(lambda x : x[1]>1)\\\n",
    "                .keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[610001457412, 610000865672, 610004533736, 610002128460, 610001686448, 610000610732, 610002220436, 610001089492, 610001540072, 610002334912, 610001724620, 610001335300, 610002282224, 610004500700, 610003446461, 610003257545, 610001229529, 610001620153, 610002024609, 610003910817, 610004246593, 610001962533, 610000251325, 610003061373, 610002387461, 610000844977, 610001937373, 610002897873, 610004193941, 610001126017, 610002031806, 610000765018, 610001727882, 610001640446, 610000969474, 610002995654, 610003238642, 610002436134, 610003952894, 610002659554, 610004589183, 610003261295, 610002738567, 610002755339, 610000345051, 610003373007, 610000481591, 610001098163, 610004285055, 610003338135, 610002428611, 610001391075, 610004165555, 610002019347, 610004360383]\n"
     ]
    }
   ],
   "source": [
    "print(NonUnique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[42] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = rdd.filter(lambda x : int (x.idnt_comp_serv)  not in NonUnique)\n",
    "rdd.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modèle comme le Random Forest (basés sur les arbres) est robuste aux valeurs manquantes !\n",
    "Imputer ici risque de créer des données artificielles et eleminer certaines rélations \n",
    "Une solution est de considerer la valeur manquante comme une modalité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cas des variables quantitatives et valeurs manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici assongons -1 lorsqu'il s'agit de NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions\n",
    "def MissingRecode(arr) :\n",
    "    return [-1 if v is None else v for v in arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récuperations des Champs utiles\n",
    "train = rdd.map(lambda x: (x.idnt_comp_serv, (x.idnt_comp_serv, x.flg_rsil, x.anc_actv, x.val_debit_down, x.iad_anc_info, \n",
    "                                              x.nb_iad, x.fdate_iad_anc, x.rk_asc_iad_conf,x.stb_anc_info, x.nb_stv,\n",
    "                                              x.fdate_stb_anc,  x.rk_asc_stb_conf, x.noinadinfo ,x.nostbinfo)\n",
    "                          ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.map(lambda x : (x[0],MissingRecode(x[1])))\n",
    "train =train.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# nombre totales de features\n",
    "NQuantvar = train.map(lambda x : len(x[1])).first()\n",
    "print NQuantvar-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement des variables catégorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA.join(B) => (KEY, (VA,VB))\\nC.join(D) => (KEY, ((VA,VB),VC))\\nD.join(E) => (KEY, (((VA,VB),VC),VD))\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def JoinListAppender(aList,aValue):\n",
    "    aList.append(aValue)\n",
    "    return tuple(aList)\n",
    "\n",
    "'''\n",
    "A.join(B) => (KEY, (VA,VB))\n",
    "C.join(D) => (KEY, ((VA,VB),VC))\n",
    "D.join(E) => (KEY, (((VA,VB),VC),VD))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On rappelle que df est un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('idnt_comp_serv', 'bigint'),\n",
       " ('flg_rsil', 'int'),\n",
       " ('anc_actv', 'int'),\n",
       " ('val_debit_down', 'float'),\n",
       " ('libl_res', 'string'),\n",
       " ('libl_tech', 'string'),\n",
       " ('nb_iad', 'int'),\n",
       " ('iad_anc_info', 'int'),\n",
       " ('iad_userkey2', 'string'),\n",
       " ('fdate_iad_anc', 'int'),\n",
       " ('iad_model', 'string'),\n",
       " ('iad_vendor', 'string'),\n",
       " ('iad_softwarever', 'string'),\n",
       " ('rk_asc_iad_conf', 'int'),\n",
       " ('nb_stv', 'int'),\n",
       " ('stb_anc_info', 'int'),\n",
       " ('fdate_stb_anc', 'int'),\n",
       " ('stb_model', 'string'),\n",
       " ('stb_vendor', 'string'),\n",
       " ('stb_softwarever', 'string'),\n",
       " ('rk_asc_stb_conf', 'int'),\n",
       " ('noinadinfo', 'int'),\n",
       " ('nostbinfo', 'int')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.index('noinadinfo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encodage des variables catégorielles (de texte)  en entier (integer)\n",
    "for i in df.dtypes:\n",
    "    if i[1]=='string':\n",
    "        # Recuperer l'index de la colonne\n",
    "        j = int(df.columns.index(i[0]))\n",
    "        #  Recuperer la list des valeurs uniques de chaque variable categorielle\n",
    "        valList = rdd.map(lambda x : (x[j],1)).groupByKey().sortByKey().map(lambda x :x[0]).collect()\n",
    "        # Retourner la clée et la colonne (nouvelle )\n",
    "        recode = rdd.map(lambda x : (x.idnt_comp_serv,valList.index(x[j])))\n",
    "        #  jointure à la table rdd de la colum\n",
    "        train = train.join(recode).map(lambda x: (x[0],JoinListAppender(list((x[1][0])),x[1][1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.unpersist()\n",
    "train=train.repartition(4).persist()\n",
    "train.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse prédictive avec RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import DenseVector\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation de la table pour MLlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LabeledPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On parle de vecteur parse lorsque vous avez beaucoup de valeurs dans le vecteur comme zéro Alors qu'on parle de vcteur dense lorsque la plupart des valeurs du vecteur sont non nulles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**vecteur dense**\n",
    "\n",
    "vd= [0,1,0,3,4,0,5,0,0,0,0,0,0,0,0,0,0,0,0,12]\n",
    "\n",
    "\n",
    "**vecteur sparse** --> Optimisation de memoire'''\n",
    "\n",
    "vs = [{1:1},{3:3},{6:5},{19:12}]            # couple indice valeurs, les indices non présentes ont par défintions des  valeurs\n",
    "nulles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "Nvar=train.map(lambda x: len(x[1])).first()\n",
    "print Nvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour entrainer  un nombre , nous devrions instancier une Classe labeledpoint qui représente pour chaque individu le labels (indice =0) et  features indice 1 , dont le contenu est  une liste).\n",
    "\n",
    "Afin d'identifier les individus la notion de clé valeurs (ici le labeledpoint) reste d'actualité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(610004152320,\n",
       "  LabeledPoint(1.0, [5.0,6144.0,4.0,1.0,4.0,1.0,-1.0,-1.0,-1.0,-1.0,0.0,0.0,5.0,3.0,1.0,2.0,2.0,80.0,0.0,0.0,47.0])),\n",
       " (610000699400,\n",
       "  LabeledPoint(0.0, [53.0,20000.0,5.0,1.0,5.0,4.0,0.0,1.0,0.0,3.0,0.0,0.0,2.0,3.0,2.0,9.0,4.0,42.0,8.0,3.0,39.0])),\n",
       " (610001369800,\n",
       "  LabeledPoint(0.0, [42.0,20000.0,8.0,2.0,8.0,4.0,1.0,2.0,1.0,7.0,0.0,0.0,5.0,3.0,2.0,2.0,2.0,80.0,7.0,3.0,74.0]))]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp =train.map(lambda x : (x[0],LabeledPoint(float(x[1][1]),DenseVector([x[1][i] for i in range(2,Nvar)])))) \n",
    "lp=lp.persist()\n",
    "lp.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([5.0, 6144.0, 4.0, 1.0, 4.0, 1.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 5.0, 3.0, 1.0, 2.0, 2.0, 80.0, 0.0, 0.0, 47.0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp.map(lambda x : (x[1].features)).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp.map(lambda x : (x[1].label)).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Déclarations de features catégorielles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous dévrions informer à Spark les features qui sont catégorielles afin qu'il puisse les traiter differemment.\n",
    "\n",
    "Pour cela , il faut créer un dictionnaire python qui retournera les features et leurs nombres de modalités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{16: 6, 17: 95, 18: 9, 19: 5, 20: 77, 13: 4, 14: 4, 15: 12}\n"
     ]
    }
   ],
   "source": [
    "catFeatInfo = {} # on a créer un dictionnaire pour que spark comprenne quelle sont les var text et numérique\n",
    "for i in range(NQuantvar-1,Nvar-2):\n",
    "    catBins =lp.map(lambda x: x[1].features[i]).distinct().count()\n",
    "    catFeatInfo.update(dict({i:catBins}))\n",
    "print catFeatInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation d'un échantillon train et test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N apprentissage 55026 N -Test 13589\n"
     ]
    }
   ],
   "source": [
    "(lpTrain,lpTest)=lp.randomSplit([0.8,0.2])\n",
    "print(\"N apprentissage %d N -Test %d\" %(lpTrain.count(),lpTest.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpTrain.persist()\n",
    "RNFModel = RandomForest.trainRegressor(lpTrain.map(lambda x: x[1]),\n",
    "                                      categoricalFeaturesInfo=catFeatInfo,\n",
    "                                      numTrees=10, featureSubsetStrategy=\"sqrt\",\n",
    "                                      impurity='variance',maxDepth=4,maxBins=95,\n",
    "                                      seed=2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde du modèle sur hdfs et importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exportation\n",
    "RNFModel.save(sc,\"/user/hadoop/MODELS/RNF_models_2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeEnsembleModel regressor with 10 trees\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Importation\n",
    "from pyspark.mllib.tree import *\n",
    "Model = RandomForestModel.load(sc, \"/user/hadoop/MODELS/RNF_models_2022\")\n",
    "print Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|flg_rsil|count|\n",
      "+--------+-----+\n",
      "|       0|60230|\n",
      "|       1| 8495|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"flg_rsil\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 13589, mean: 0.125067091281, stdev: 0.063807783476, max: 0.547158612926, min: 0.0792222281688)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply model to test\n",
    "Preds = RNFModel.predict(lpTest.map(lambda x: x[1].features))\n",
    "Preds.stats()\n",
    "#Taux d'évènement à 12% donc c'est bien ça rpz notre % de churner dans notre table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 13589, mean: 0.125067091281, stdev: 0.063807783476, max: 0.547158612926, min: 0.0792222281688)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Preds.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.15557054325603212, 0.10340656299462155]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Preds.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Courbe Lift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le lift a un rôle de ciblage : solliciter les clients les plus réceptifs\n",
    "\n",
    "• optimiser un budget limité\n",
    "\n",
    "• ne pas agacer les clients « hostiles »\n",
    "\n",
    "Les étapes :\n",
    "\n",
    "\n",
    "1. Appliquer la fonction score sur le reste de la base\n",
    "2. Trier la base selon le score\n",
    "3. Cibler en priorité les clients à fort score(les plus appétents en premier. Les moins appétents en derniers)\n",
    "4. Prévoir les performances à partir de la courbe LIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rattachons les individus à leur prédictions\n",
    "\n",
    "- x[0][0] = id_client\n",
    "- x[0][1] =  label\n",
    "- x[1] = prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((610003865040, 0.0), 0.15557054325603212, 0.15557054325603212),\n",
       " ((610004355800, 0.0), 0.10340656299462155, 0.10340656299462155),\n",
       " ((610000699680, 0.0), 0.11092487213073658, 0.11092487213073658)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kPreds=lpTest.map(lambda x : (x[0], x[1].label)).zip(Preds).zip(Preds).map(lambda x : (x[0][0],x[0][1], x[1]))\n",
    "kPreds.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((610003865040, 0.0), 0.15557054325603212)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lpTest.map(lambda x : (x[0], x[1].label)).zip(Preds).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13589, 1699.5367034235016)\n"
     ]
    }
   ],
   "source": [
    "Nrows = kPreds.count()\n",
    "Tres = kPreds.map(lambda x : x[1]).reduce(lambda x,y :x+y)\n",
    "print(Nrows, Tres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordonnées les probabilités : ((0.54436346, 0.54436346054), 0)\n",
    "# Fréquence d'individus, (1, probabilité)\n",
    "# Par fréquence de 1%: compter le nombre d'individus, somme des probabilité\n",
    "# Par fréquence de 1%: compter le nombre d'individus, somme des probabilité , le ratio somme des proba/ somme totale des proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = kPreds.map(lambda x: (x[2], x[1])).sortByKey(ascending=False).zipWithIndex()\\\n",
    "               .map(lambda x : (x[1]*100/Nrows, (1, x[0][1]))).reduceByKey(lambda x,y: (x[0] + y[0], x[1] + y[1]))\\\n",
    "               .map(lambda x : (x[0], x[1][0], x[1][1], x[1][1]*1.00/Tres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 136, 66.94210795021455, 0.0393884449893716),\n",
       " (1, 136, 63.25660181938903, 0.037219909221122796),\n",
       " (2, 136, 42.98631397385356, 0.02529296006803682),\n",
       " (3, 136, 32.575858563422855, 0.01916749341029406),\n",
       " (4, 136, 30.8326218431235, 0.01814178051054448),\n",
       " (5, 136, 29.854704098859372, 0.017566377965666083),\n",
       " (6, 136, 28.179434267943204, 0.016580656487841246),\n",
       " (7, 136, 26.365524067084095, 0.015513359619697582),\n",
       " (8, 136, 25.929821847968384, 0.01525699433012305),\n",
       " (9, 135, 25.625829302709167, 0.015078126439452102),\n",
       " (10, 136, 24.945528141423615, 0.014677840196786574),\n",
       " (11, 136, 24.736034691118437, 0.01455457516233149),\n",
       " (12, 136, 23.80569775570839, 0.014007168958313653),\n",
       " (13, 136, 23.238738748857973, 0.013673572746058661),\n",
       " (14, 136, 22.96619280690055, 0.013513207899916526),\n",
       " (15, 136, 21.38477451501969, 0.012582708259223098),\n",
       " (16, 136, 20.9859138711442, 0.012348020392187312),\n",
       " (17, 136, 20.378066340385047, 0.011990365550409127),\n",
       " (18, 135, 19.30877672252256, 0.011361200192751043),\n",
       " (19, 136, 18.771255173275144, 0.01104492485243939),\n",
       " (20, 136, 18.439714343491133, 0.010849847671042738),\n",
       " (21, 136, 17.76892915269804, 0.01045516058400197),\n",
       " (22, 136, 17.05946900072171, 0.010037717318112382),\n",
       " (23, 136, 16.04852302785931, 0.009442881107263875),\n",
       " (24, 136, 15.326830487305678, 0.009018240357170115),\n",
       " (25, 136, 15.085782609780152, 0.008876408835061786),\n",
       " (26, 136, 15.085782609780152, 0.008876408835061786),\n",
       " (27, 135, 14.974857737649415, 0.008811141123039273),\n",
       " (28, 136, 15.056837295574157, 0.008859377538151464),\n",
       " (29, 136, 14.95383522593634, 0.008798771568636165),\n",
       " (30, 136, 14.420171146937674, 0.008484765947031367),\n",
       " (31, 136, 14.170993369129032, 0.008338150826977352),\n",
       " (32, 136, 14.130649725623261, 0.008314412802711972),\n",
       " (33, 136, 14.102157503111584, 0.008297648102982755),\n",
       " (34, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (35, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (36, 135, 13.959886004273883, 0.008213936172224736),\n",
       " (37, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (38, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (39, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (40, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (41, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (42, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (43, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (44, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (45, 135, 13.959886004273883, 0.008213936172224736),\n",
       " (46, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (47, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (48, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (49, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (50, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (51, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (52, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (53, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (54, 135, 13.959886004273883, 0.008213936172224736),\n",
       " (55, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (56, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (57, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (58, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (59, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (60, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (61, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (62, 136, 14.063292567268505, 0.008274780143870845),\n",
       " (63, 135, 13.930569776247358, 0.008196686631236611),\n",
       " (64, 136, 13.948260606310296, 0.008207095838656081),\n",
       " (65, 136, 13.948260606310296, 0.008207095838656081),\n",
       " (66, 136, 13.948260606310296, 0.008207095838656081),\n",
       " (67, 136, 13.948260606310296, 0.008207095838656081),\n",
       " (68, 136, 13.948260606310296, 0.008207095838656081),\n",
       " (69, 136, 13.948260606310296, 0.008207095838656081),\n",
       " (70, 136, 13.948260606310296, 0.008207095838656081),\n",
       " (71, 136, 13.948260606310296, 0.008207095838656081),\n",
       " (72, 135, 13.709827918029843, 0.008066803082518389),\n",
       " (73, 136, 13.386200912206164, 0.007876382360699452),\n",
       " (74, 136, 13.386200912206164, 0.007876382360699452),\n",
       " (75, 136, 13.386200912206164, 0.007876382360699452),\n",
       " (76, 136, 13.386200912206164, 0.007876382360699452),\n",
       " (77, 136, 13.340471808909722, 0.00784947555533048),\n",
       " (78, 136, 13.271168951248015, 0.007808698055484724),\n",
       " (79, 136, 13.271168951248015, 0.007808698055484724),\n",
       " (80, 136, 13.269354233327974, 0.007807630283358128),\n",
       " (81, 135, 13.170445968667954, 0.007749433090875741),\n",
       " (82, 136, 13.268004827695124, 0.007806836298956303),\n",
       " (83, 136, 13.268004827695124, 0.007806836298956303),\n",
       " (84, 136, 13.268004827695124, 0.007806836298956303),\n",
       " (85, 136, 13.268004827695124, 0.007806836298956303),\n",
       " (86, 136, 13.268004827695124, 0.007806836298956303),\n",
       " (87, 136, 13.268004827695124, 0.007806836298956303),\n",
       " (88, 136, 13.161628039927153, 0.007744244659979816),\n",
       " (89, 136, 12.59091317263279, 0.007408438515784913),\n",
       " (90, 135, 12.006121443545926, 0.007064349607373064),\n",
       " (91, 136, 11.898140769262435, 0.007000814248550876),\n",
       " (92, 136, 11.898140769262435, 0.007000814248550876),\n",
       " (93, 136, 11.898140769262435, 0.007000814248550876),\n",
       " (94, 136, 11.898140769262435, 0.007000814248550876),\n",
       " (95, 136, 11.898140769262435, 0.007000814248550876),\n",
       " (96, 136, 11.898140769262435, 0.007000814248550876),\n",
       " (97, 136, 11.87614936496164, 0.006987874601965724),\n",
       " (98, 136, 11.572729207924572, 0.006809343502033686),\n",
       " (99, 135, 11.018798007421271, 0.006483412794337008)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.takeOrdered(100, lambda x : x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19206103042\n"
     ]
    }
   ],
   "source": [
    "cutOff = 10\n",
    "Lift = res.filter(lambda x :x[0]<cutOff).map(lambda x: x[3]).sum()/(cutOff/100.00)\n",
    "print Lift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIMISATION DES PARAMETRE DU MODELE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTrees: 10 - Depth : 6 - Lift : 2.659548 - (3.850312 s.)\n",
      "numTrees: 10 - Depth : 8 - Lift : 2.759576 - (4.911552 s.)\n",
      "numTrees: 10 - Depth : 10 - Lift : 2.777227 - (6.189529 s.)\n",
      "numTrees: 10 - Depth : 16 - Lift : 2.788995 - (11.455336 s.)\n",
      "numTrees: 20 - Depth : 6 - Lift : 2.594825 - (5.191044 s.)\n",
      "numTrees: 20 - Depth : 8 - Lift : 2.741924 - (6.299267 s.)\n",
      "numTrees: 20 - Depth : 10 - Lift : 2.853719 - (11.574427 s.)\n",
      "numTrees: 20 - Depth : 16 - Lift : 2.806647 - (29.025064 s.)\n",
      "numTrees: 50 - Depth : 6 - Lift : 2.665432 - (10.191149 s.)\n",
      "numTrees: 50 - Depth : 8 - Lift : 2.724272 - (15.906474 s.)\n",
      "numTrees: 50 - Depth : 10 - Lift : 2.894907 - (24.902319 s.)\n",
      "numTrees: 50 - Depth : 16 - Lift : 2.877255 - (70.924941 s.)\n"
     ]
    }
   ],
   "source": [
    "# Optimisation des paramètres du modele\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#lpTrain.persist()\n",
    "cutOff = 10\n",
    "\n",
    "for i in (10,20,50):\n",
    "    for j in (6, 8, 10, 16):\n",
    "        start_time = time.time()\n",
    "        RNFModel = RandomForest.trainRegressor(lpTrain.map(lambda x: x[1]),\n",
    "                                               categoricalFeaturesInfo= catFeatInfo,\n",
    "                                               numTrees=i,\n",
    "                                               featureSubsetStrategy=\"sqrt\",\n",
    "                                               impurity='variance',\n",
    "                                               maxDepth=j,\n",
    "                                               maxBins=95,\n",
    "                                               seed=12345)\n",
    "        Preds = RNFModel.predict(lpTest.map(lambda x: x[1].features))\n",
    "        kPreds = lpTest.map(lambda x: (x[0], x[1].label)).zip(Preds).map(lambda x : (x[0][0], x[0][1], x[1]))\n",
    "        res = kPreds.map(lambda x: (x[2], x[1])).sortByKey(ascending=False).zipWithIndex()\\\n",
    "               .map(lambda x : (x[1]*100/Nrows,(1, x[0][1]))).reduceByKey(lambda x,y : (x[0]+ y[0], x[1]+y[1]))\\\n",
    "               .map(lambda x : (x[0], x[1][0], x[1][1], x[1][1]*1.00/Tres))\n",
    "        Lift = res.filter(lambda x :x[0] < cutOff).map(lambda x: x[3]).sum()/(cutOff/100.00)\n",
    "        elapsed = time.time() - start_time\n",
    "        print (\"numTrees: %d - Depth : %d - Lift : %f - (%f s.)\" %(i,j,Lift,elapsed))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
