Big O notation is a critical aspect of data structures and algorithms that every software engineer should know.

It's written O(x) where x is usually a function that grows with the amount of data you pass the algorithm.

O(1) means that the algorithm takes the same amount of time regardless of the input data size.
O(n) means the algorithm's time scales linearly with the input data size.

Things that are O(1):
Accessing data in a hashmap with its key, jumping to the next node of a linked list, checking the membership of data in a set.

Things that are O(log n):
Looking up a database field by its index.

Things that are O(n):
Iterating over an array, the best case for some sort sorting algorithms like bubble sort

Things that are O(n log n):
A lot of sorting algorithms. Like quick sort, merge sort etc.


Knowing how to minimize the big O of an algorithm is one of the things that separates the good software engineers from the great ones.


Thanks to Zach Wilson
